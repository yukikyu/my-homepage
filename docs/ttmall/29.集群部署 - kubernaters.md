# 29.集群部署 - kubernaters

## 简介

Kubernetes简称k8s。是用于自动部署，扩展和管理容器化应用程序的开源系统。

[中文官网](https://kubernetes.io/zh/)

- **服务发现和负载均衡**

  Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。

- **存储编排**

  Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。

- **自动部署和回滚**

  你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。

- **自动完成装箱计算**

  Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。

- **自我修复**

  Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。

- **密钥与配置管理**

  Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。

## 架构原理&核心概念

![](\img\components-of-kubernetes.svg)

### 整体主从方式

![](\img\Kubernetes cluster.png)

### 控制平面组件（Control Plane Components）

#### kube-apiserver

API 服务器是 Kubernetes [控制面](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane)的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。

Kubernetes API 服务器的主要实现是 [kube-apiserver](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。

#### etcd

etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。

您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。

要了解 etcd 更深层次的信息，请参考 [etcd 文档](https://etcd.io/docs/)。

#### kube-scheduler

控制平面组件，负责监视新创建的、未指定运行[节点（node）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)的 [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)，选择节点让 Pod 在上面运行。

调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。

#### kube-controller-manager

运行[控制器](https://kubernetes.io/zh/docs/concepts/architecture/controller/)进程的控制平面组件。

从逻辑上讲，每个[控制器](https://kubernetes.io/zh/docs/concepts/architecture/controller/)都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。

这些控制器包括:

- 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应
- 任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成
- 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)
- 服务帐户和令牌控制器（Service Account & Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌

#### cloud-controller-manager

云控制器管理器是指嵌入特定云的控制逻辑的 [控制平面](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane)组件。 云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。

`cloud-controller-manager` 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。

与 `kube-controller-manager` 类似，`cloud-controller-manager` 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。

下面的控制器都包含对云平台驱动的依赖：

- 节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除
- 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由
- 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器

### Node 组件 

节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。

#### kubelet

一个在集群中每个[节点（node）](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)上运行的代理。 它保证[容器（containers）](https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/#why-containers)都 运行在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 中。

kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。

#### kube-proxy

[kube-proxy](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-proxy/) 是集群中每个节点上运行的网络代理， 实现 Kubernetes [服务（Service）](https://kubernetes.io/zh/docs/concepts/services-networking/service/) 概念的一部分。

kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。

如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。

#### 容器运行时（Container Runtime） 

容器运行环境是负责运行容器的软件。

Kubernetes 支持多个容器运行环境: [Docker](https://kubernetes.io/zh/docs/reference/kubectl/docker-cli-to-kubectl/)、 [containerd](https://containerd.io/docs/)、[CRI-O](https://cri-o.io/#what-is-cri-o) 以及任何实现 [Kubernetes CRI (容器运行环境接口)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md)。

### 插件（Addons） 

插件使用 Kubernetes 资源（[DaemonSet](https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/)、 [Deployment](https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/)等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 `kube-system` 命名空间。

下面描述众多插件中的几种。有关可用插件的完整列表，请参见 [插件（Addons）](https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/)。

#### DNS 

尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有[集群 DNS](https://kubernetes.io/zh/docs/concepts/services-networking/dns-pod-service/)， 因为很多示例都需要 DNS 服务。

集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。

Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。

#### Web 界面（仪表盘）

[Dashboard](https://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/) 是 Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。

#### 容器资源监控

[容器资源监控](https://kubernetes.io/zh/docs/tasks/debug-application-cluster/resource-usage-monitoring/) 将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。

#### 集群层面日志

[集群层面日志](https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/) 机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。

### 概念

![](\img\KuBernetes概念.png)

- Container：容器，可以是docker启动的一个容器

- Pod:

  - k8s使用Pod来组织一组容器
  - 一个Pod中的所有容器共享同一网络。
  - Pod是k8s中的最小部署单元

- Volume

  - 声明在Pod容器中可访问的文件目录

  - 可以被挂在在Pod中一个或多个容器指定路径下

  - 支持多种后端存储抽象（本地存储，分布式存储，云存储。。。）

    ![](\img\KuBernetes概念 - 01.png)

- Controllers：更高层次对象，部署和管理Pod；

  - ReplicaSet：确保预期的Pod副本数量
  - Deplotment：无状态应用部署
  - StatefulSet：有状态应用部署
  - DaemonSet：确保所有Node都运行一个指定Pod
  - Job：一次性任务
  - Cronjob：定时任务

- Deployment：

  - 定义一组Pod的副本数目、版本等

  - 通过控制器（Controller）维持Pod数目（自动回复失败的Pod）

  - 通过控制器以指定的策略控制版本（滚动升级，回滚等）

    ![](\img\KuBernetes概念 - 02.png)

- Service

  - 定义一组Pod的访问策略

  - Pod的负载均衡，提供一个或者多个Pod的稳定访问地址

    ![](\img\KuBernetes概念 - 03.png)

  - 支持多种方式（ClusterIP、NodePort、Loadalance）

- Lable：标签，用于对象资源的查询，筛选

  ![](\img\KuBernetes概念 - 04.png)

- Namespace：命名空间，逻辑隔离

  - 一个集群内部的逻辑隔离机制（鉴权，资源）
  - 每个资源都属于一个namespace
  - 同一个namespace所有资源名不能重复
  - 不同namespace可以资源名重复

> API：
>
> 我们通过kubernetes的API来操作整个集群.
>
> 可以通过kubectl、ui、curl最终发送htto+json/yaml方式的请求给API Server，然后控制k8s集群。k8s里的所有的资源对象都可以采用yaml或JSON格式的文件定义或描述
>
> ![](\img\KuBernetes概念 - yaml.png)

## 常用命令

```shell
# 删除 master 上的污点，以便您可以在其上安排 pod
$ kubectl taint nodes --all node-role.kubernetes.io/master-
~ node/<your-hostname> untainted

# 获取节点
$ kubectl get nodes -o wide
~ NAME              STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
  <your-hostname>   Ready    master   52m   v1.12.2   10.128.0.28   <none>        Ubuntu 18.04.1 LTS   4.15.0-1023-gcp     docker://18.6.1
```



## 快速体验

### kubeadm

kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。

这个工具能通过两条指令完成一个kubernetes集群的部署：

- 创建一个Master节点

  ```shell
  $ kubeadm init
  ```

- 将一个Node节点加入到当前集群中

  ```shell
  $ kubeadm join <Master节点的IP和端口>
  ```

### 前置要求

- 一台或多台机器，操作系统CentOS7.x-86_x64

- 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多

- 集群中所有机器之间网络互通
- 可以访问外网，需要拉取镜像
- 禁止swap分区

### 部署步骤

1. 在所有节点上安装Docker和kubeadm
2. 部署kubernates Master
3. 部署容器网络插件
4. 部署Kubernates Node，将节点加入Kubernates集群中
5. 部署Dashboard Web页面，可视化查看Kubernates资源

![](\img\kuberbetes部署步骤.png)

### 准备工作

- 我们可以使用vargrant快速创建三个虚拟机。虚拟机启动前先设置virtualbox的主机网络。现全部统一为`192.168.56.1`，以后所有虚拟机都是56.x的ip地址：

  管理 => 主机网络管理器

  ![](\img\主机网络管理器.png)

### 环境准备 

- 使用Vagrant快速创建多台虚拟机：`Vagrantfile`

  ```c
  Vagrant.configure("2") do |config|
  
  	(1..3).each do |i|
  
  		config.vm.define "k8s-node#{i}" do |node|
  
  		# 设置虚拟机的Box
  		node.vm.box = "centos/7"
  
  		# 设置虚拟机的主机名
  		node.vm.hostname="k8s-node#{i}"
  
  		# 设置虚拟机的IP
  		node.vm.network "private_network", ip: "192.168.56.#{99 + i}", netmask: "255.255.255.0"
  
  		# 设置主机与虚拟机的共享目录
  		# node.vm.synced_folder "~/Desktop/share", "/home/vagrant/share"
  
  		# VirtaulBox相关配置
  		node.vm.provider "virtualbox" do |v|
  
  			# 设置虚拟机的名称
  			v.name = "k8s-node#{i}"
  
  			# 设置虚拟机的内存大小  
  			v.memory = 4096
  
  			# 设置虚拟机的CPU个数
  			v.cpus = 2
  		end
    
  		# 使用shell脚本进行软件安装和配置
  		# node.vm.provision "shell", inline: <<-SHELL
  
  			# 安装docker 1.11.0
  			# wget -qO- https://get.docker.com/ | sed 's/docker-engine/docker-engine=1.11.0-0~trusty/' | sh
  			# usermod -aG docker vagrant
  			
  		# SHELL
  
  		end
  	end
  end
  ```

- 进入三个虚拟机，开启root的密码访问权限

  ```shell
  $ su root
  # 密码 vagrant
  $ vi /etc/ssh/sshd_config
  # 修改 PasswordAuthentication yes/no
  $ service sshd restart
  ```

- 设置网络环境
  - 添加一个NAT（全局设置 => 网络 => [增加一个网络]）网络
  - 修改虚拟机为NAT并重新分配一个MAC地址

- 设置linux环境

  - 关闭防火墙

    ```shell
    $ systemctl stop firewalld
    $ systemctl disable firewalld
    ```

  - 关闭selinux

    ```shell
    $ sed -i 's/enforcing/disabled/' /etc/selinux/config
    $ cat /etc/selinux/config
    ```

  - 关闭swap

    ```shell
    # 临时
    $ swapoff -a
    # 永久
    $ sed -ri 's/.*swap.*/#&/' /etc/fstab
    $ cat /etc/fstab
    ```

  - 添加主机名与IP对应的关系

    ```shell
    $ vi /etc/hosts
    192.168.248.129 k8s-master
    192.168.248.130 k8s-node1
    192.168.248.131 k8s-node2
    
    # 指定hostname
    $ hostname
    $ hostnamectl set-hostname <newhostname>
    ```

  - 将桥接的IPv4流量传递到iptables的链

    ```shell
    $ cat > /etc/sysctl.d/k8s.conf << EOF
    net.bridge-nf-call-ip6tables = 1
    net.bridge-nf-call-iptable = 1
    EOF
    $ sysctl --system
    ```

  - 备份虚拟机

### 所有节点环境安装

#### Docker

1. 卸载系统之前的docker

   ```shell
   yum remove docker \
       docker-client \
       docker-client-latest \
       docker-common \
       docker-latest \
       docker-latest-logrotate \
       docker-logrotate \
       docker-engine
   ```

2. 安装必备插件

   ```shell
   yum install -y yum-utils \
   device-mapper-persistent-data \
   lvm2
   ```

3. 设置docker repo的yum位置

   ```shell
   yum-config-manager \
   --add-repo \
   http://download.docker.com/linux/centos/docker-ce.repo
   ```

4. 安装docker，以及docker-cli

   ```shell
   yum install -y docker-ce docker-ce-cli containerd.io
   ```

5. 配置docker加速

   ```shell
   sudo mkdir -p /etc/docker
   sudo tee /etc/docker/daemon.json <<-'EOF'
   {
     "registry-mirrors": ["https://82m9ar63.mirror.aliyuncs.com"],
     "exec-opts": ["native.cgroupdriver=systemd"],
     "log-driver": "json-file",
     "log-opts": {
       "max-size": "100m"
     },
     "storage-driver": "overlay2"
   }
   EOF
   sudo systemctl daemon-reload
   sudo systemctl restart docker
   ```

6. 开机自启

   ```shell
   systemctl enable docker
   ```

#### kubeadm、kubelet、kubectl

- 添加阿里云yum源

  ```shell
  #配置k8s的yum源地址
  cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
  [kubernetes]
  name=Kubernetes
  baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
  enabled=1
  gpgcheck=0
  repo_gpgcheck=0
  gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
     http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
  EOF
  ```

  

- 安装kubeadm，kubelet和kubectl

  ```shell
  yum list|grep kube
  # 版本依据官网版本
  yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes
  
  # 卸载
  yum remove -y kubelet kubeadm kubectl --disableexcludes=kubernetes
  ```

- 开启kubelet

  ```shell
  systemctl enable --now kubelet
  systemctl start kubelet
  ```


### 部署k8s

#### master

1. master节点初始化

   ```shell
   #所有机器添加master域名映射，以下需要修改为自己的
   echo "192.168.248.138 cluster-endpoint" >> /etc/hosts
   
   #主节点初始化
   kubeadm init \
   --apiserver-advertise-address=192.168.248.138 \
   --control-plane-endpoint=cluster-endpoint \
   --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \
   --kubernetes-version v1.20.9 \
   --service-cidr=10.96.0.0/16 \
   --pod-network-cidr=192.168.0.0/16 # 配置calico中默认得ip:192.168.0.0/16
   ```

   由于默认拉去镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。可以手动按照我们images.sh先拉取镜像，地址变为`registry.aliyuncs.com/google_containers`也可以。

   科普：五类别域间路由（Classless Inter-Domain Routing、CIDR）是一个用于给用户分配IP地址以及在互联网上有效的路由IP数据包的对IP地址进行归类的方法。

   拉取可能失败，需要下载镜像。

   `master_images.sh`

   ```sh
   sudo tee ./images.sh <<-'EOF'
   #!/bin/bash
   images=(
   kube-apiserver:v1.20.9
   kube-proxy:v1.20.9
   kube-controller-manager:v1.20.9
   kube-scheduler:v1.20.9
   coredns:1.7.0
   etcd:3.4.13-0
   pause:3.2
   )
   for imageName in ${images[@]} ; do
   docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName
   done
   EOF
      
   chmod +x ./images.sh && ./images.sh
   ```

   

   运行完成提前复制：加入集群的令牌

   

2. 测试kubelet（主节点执行）

   ```shell
   $ mkdir -p $HOME/.kube
   $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   $ sudo chown $(id -u):$(id -g) $HOME/.kube/config
   
   # 获取所有节点
   $ kubectl get nodes
   
   ```

   目前master状态为notready。等待网络加入完成即可。

   ```shell
   # 查看kubelet日志
   $ journalctl -u kubelet
   # 两小时失效↓↓↓
   $ kubeadm join 10.0.2.5:6443 --token 8u48py.jkone4j4j45e0wd4 \
   	--discovery-token-ca-cert-hash sha256:d5c58aa170a0fd80ee39898373a5aea4090eb1ef3d3a90645d447a03a484ccca
   ```

   

3. 安装pod网络插件

   ```shell
   curl https://docs.projectcalico.org/manifests/calico.yaml -O
   # 如果init中不是192.168.0.0/16 需要打开配置
   kubectl apply -f calico.yaml
   ```

   

4. 查看pod

   ```shell
   # 获取命名空间
   kubectl get namespaces
   # 获取所有pods
   kubectl get pods --all-namespaces
   # 获取所有的nodes
   kubectl get nodes
   ```

#### node

1. 使用之前初始化产生的命令将其他node加入进来

   ```shell
   $ kubeadm join 10.0.2.5:6443 --token 8u48py.jkone4j4j45e0wd4 \
   	--discovery-token-ca-cert-hash sha256:d5c58aa170a0fd80ee39898373a5aea4090eb1ef3d3a90645d447a03a484ccca
   ```

   > 获取新令牌：`kubeadm token create --print-join-command`

2. 监控网络

   ```shell
   # 如果网络出现问题，关闭cni0,重启虚拟机继续测试
   ip link set cni0 down
   # 监控pod进度等待3-10分钟，完全都是running以后继续
   watch kubectl get pod -n kube-system -o wide
   # 查看所有节点所有节点部署完毕则搭建完成
   kubectl get nodes
   ```



## 入门操作 kubernetes集群

### 部署一个tomcat

```shell
# 创建一个测试环境的tomcat
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8
# 获取k8s中所有资源 （获取更多内容 -o wide）
kubectl get all
# 查看指定pod运行在哪个节点 在指定节点查看docker运行状态
docker images
docker ps
# 获取默认名称空间的pods （获取更多内容 -o wide）
kubectl get pods
# 模拟宕机 将pod运行的node关机 发现会<容灾恢复>
```



### 暴露nginx访问

```shell
# 使用k8s暴露端口
kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort
# 获取service （获取更多内容 -o wide）
kubectl get svc
```



### 动态扩容测试

```shell
kubectl get deployment
# 应用升级（--help 查看帮助）
kubectl set image
# 扩容/降级 （根据数量增加部署量）
kubectl scale --replicas=3 deployment tomcat6
```

### 删除

```shell
kubectl get all
kubectl delete deploy/nginx
kubectl delete service/nginx-service
```

> 流程：创建deployment会管理replicas控制pod数量，有pod故障会自动拉起新的pod



### yaml&基本使用

[kubectl文档](https://kubernetes.io/zh/docs/reference/kubectl/kubectl/)

[kubectl命令](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands)

[资源类型](https://kubernetes.io/zh/docs/reference/kubectl/overview/#%E8%B5%84%E6%BA%90%E7%B1%BB%E5%9E%8B)

[格式化输出](https://kubernetes.io/zh/docs/reference/kubectl/overview/#%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA)

[常用操作](https://kubernetes.io/zh/docs/reference/kubectl/overview/#%E7%A4%BA%E4%BE%8B-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C)

#### yml模板

![](\img\KuBernetes概念 - yaml.png)

```shell
# 生成yaml(> tomcat6.yml 输入到文件中)
kubectl create deployment tomcat6 --image=tomcat:6.0.53-hre8 --dry-run -o yaml
# pod反向生成yaml
kubectl get pod [pod名] -o yaml
```

#### 

#### yml字段解析

[参照官方文档](找不到)

### pod、service概念

[pod](https://kubernetes.io/zh/docs/concepts/workloads/pods/)

#### Pod 和控制器 

你可以使用工作负载资源来创建和管理多个 Pod。 资源的控制器能够处理副本的管理、上线，并在 Pod 失效时提供自愈能力。 例如，如果一个节点失败，控制器注意到该节点上的 Pod 已经停止工作， 就可以创建替换性的 Pod。调度器会将替身 Pod 调度到一个健康的节点执行。

下面是一些管理一个或者多个 Pod 的工作负载资源的示例：

- [Deployment](https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/)
- [StatefulSet](https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/)
- [DaemonSet](https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/)

#### service

将运行在一组 [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 上的应用程序公开为网络服务的抽象方法。

使用 Kubernetes，你无需修改应用程序即可使用不熟悉的服务发现机制。 Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。

![](\img\Ingress示例.png)

### Ingress

Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。

Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。

#### 搭建环境

```shell
# 1. 删除所有pod
# 2. 使用yaml方式安装
$ kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --dry-run -o yaml > deployment-tomcat6.yml
$ kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml
# 3. 合并yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tomcat6
  template:
    metadata:
      labels:
        app: tomcat6
    spec:
      containers:
      - image: tomcat:6.0.53-jre8
        name: tomcat
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat6
  type: NodePort
# 4. 启动yaml
$ kubectl apply -f deployment-tomcat6.yml
# 5. 测试 访问端口
```

#### 部署ingress-controller

参照[官方文档](https://kubernetes.github.io/ingress-nginx/deploy/)

```shell
# 拉取yaml
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.2/deploy/static/provider/aws/deploy.yaml
# 修改镜像
sed -i 's@k8s.gcr.io/ingress-nginx/controller:v1.0.0\(.*\)@willdockerhub/ingress-nginx-controller:v1.0.0@' deploy.yaml
sed -i 's@k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.0\(.*\)$@hzde0128/kube-webhook-certgen:v1.0@' deploy.yaml
# 应用
kubectl apply -f deploy.yaml
```

#### 使用

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tomcat
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host: tomcat6.tete.cn
    http:
      paths:
      - path: "/"
        pathType: Prefix
        backend:
          service:
            name: tomcat6
            port:
              number: 80
```



## 安装默认dashboard

### 部署dashboard

参考[官方文档](https://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/)

`dashbaord.yaml`

```yaml
# Copyright 2017 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: Namespace
metadata:
  name: kubernetes-dashboard

---

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kubernetes-dashboard
type: Opaque

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-csrf
  namespace: kubernetes-dashboard
type: Opaque
data:
  csrf: ""

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-key-holder
  namespace: kubernetes-dashboard
type: Opaque

---

kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-settings
  namespace: kubernetes-dashboard

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
rules:
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
    verbs: ["get", "update", "delete"]
    # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
    verbs: ["get", "update"]
    # Allow Dashboard to get metrics.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
    verbs: ["get"]

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
rules:
  # Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.3.1
          imagePullPolicy: Always
          ports:
            - containerPort: 8443
              protocol: TCP
          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            # Uncomment the following line to manually specify Kubernetes API server Host
            # If not specified, Dashboard will attempt to auto discover the API server and connect
            # to it. Uncomment only if the default does not work.
            # - --apiserver-host=http://my-address:port
          volumeMounts:
            - name: kubernetes-dashboard-certs
              mountPath: /certs
              # Create on-disk volume to store exec logs
            - mountPath: /tmp
              name: tmp-volume
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 8443
            initialDelaySeconds: 30
            timeoutSeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      volumes:
        - name: kubernetes-dashboard-certs
          secret:
            secretName: kubernetes-dashboard-certs
        - name: tmp-volume
          emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 8000
      targetPort: 8000
  selector:
    k8s-app: dashboard-metrics-scraper

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: dashboard-metrics-scraper
  template:
    metadata:
      labels:
        k8s-app: dashboard-metrics-scraper
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      containers:
        - name: dashboard-metrics-scraper
          image: kubernetesui/metrics-scraper:v1.0.6
          ports:
            - containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              scheme: HTTP
              path: /
              port: 8000
            initialDelaySeconds: 30
            timeoutSeconds: 30
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      volumes:
        - name: tmp-volume
          emptyDir: {}
```



### 设置访问端口

```shell
kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
# 将 type: ClusterIP 修改为 type: NodePort
kubectl get svc -A |grep kubernetes-dashboard
```

### 创建系统账号

[文档地址](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

```shell
kubectl apply -f dash.yaml
```

### 获取访问令牌

```shell
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
```

内容入下

```
eyJhbGciOiJSUzI1NiIsImtpZCI6IjhjQUtLM3pHYW5iX1FSbDBmV1Y3SXgweFhTUXR6Qi16ekQxYnFWUldTTmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNxZ3h2Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwMjA5MTg0Ny0yODYxLTRlNjUtOTY2ZC04NzBjMzM0YjJlNmIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.ln3Rf1lDKtJ1eDizlrFFVQ0Aocvnf2RrYyxZpEJwCfnUW7wDGYvO2UaMLqV-Sz2KXccTJVtUz4Th-ikIZDYshZD0-dbQma3jSj0VXCm7CIJK0OQjU8S2tOOlrlGWATDeqxlfsB4FEFjHsX_cGVH3i5DnhLTQiF6gopTXPsgIX43Pzzc_bSj9yMwD4X_yLQdTsBicLllYvPJe2_SfEOQh_fcJJJNl7qX9-8mM2w2l8JChTwvyPtxZ2hJbzbNit_m8EIitPjr2643e0qFGLvX2vNrt9e2c40Ls1rkv4Cw7I9GQfQbCvWohghuzrSs9DSL29ygGP5l
```
